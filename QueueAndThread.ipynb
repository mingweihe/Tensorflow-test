{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "1\n",
      "11\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "q=tf.FIFOQueue(2,'int32')\n",
    "init=q.enqueue_many(([0,10],))\n",
    "x=q.dequeue()\n",
    "y=x+1\n",
    "q_inc=q.enqueue([y])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for _ in range(5):\n",
    "        v,_=sess.run([x,q_inc])\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9285777\n",
      "-0.9389898\n",
      "0.49509484\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "\n",
    "#def MyLoop(coord,worker_id):\n",
    "#    while not coord.should_stop():\n",
    "#        if np.random.rand()<0.1:\n",
    "#            print('stopping from id:{} \\n'.format(worker_id))\n",
    "#            coord.request_stop()\n",
    "#        else:\n",
    "#            print('working from id:{} \\n'.format(worker_id))\n",
    "#        time.sleep(1)\n",
    "#\n",
    "#coord=tf.train.Coordinator()\n",
    "#threads=[threading.Thread(target=MyLoop,args=(coord,i,)) for i in range(5)]\n",
    "#for t in threads:\n",
    "#    t.start()\n",
    "#coord.join(threads)\n",
    "\n",
    "queue=tf.FIFOQueue(100,'float')\n",
    "enqueue_op=queue.enqueue(tf.random_normal([1]))\n",
    "qr=tf.train.QueueRunner(queue,[enqueue_op]*5)\n",
    "tf.train.add_queue_runner(qr)\n",
    "out_tensor=queue.dequeue()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads=tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    for _ in range(3):\n",
    "        print(sess.run(out_tensor)[0])\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "num_shards=2\n",
    "instances_per_shard=2\n",
    "for i in range(num_shards):\n",
    "    filename=('Records/data.tfrecords-%.5d-of-%.5d' % (i,num_shards))\n",
    "    writer=tf.python_io.TFRecordWriter(filename)\n",
    "    for j in range(instances_per_shard):\n",
    "        example=tf.train.Example(features=tf.train.Features(feature={'i':_int64_feature(i),\n",
    "                                                                     'j':_int64_feature(j)}))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Records/data.tfrecords-00001-of-00002'\n",
      " b'Records/data.tfrecords-00000-of-00002']\n",
      "[1, 0]\n",
      "[1, 1]\n",
      "[0, 0]\n",
      "[0, 1]\n",
      "[1, 0]\n",
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "files=tf.train.match_filenames_once('Records/data.tfrecords-*')\n",
    "filename_queue=tf.train.string_input_producer(files,shuffle=False)\n",
    "\n",
    "reader=tf.TFRecordReader()\n",
    "_,serialized_example=reader.read(filename_queue)\n",
    "features=tf.parse_single_example(serialized_example,features={'i':tf.FixedLenFeature([],tf.int64),\n",
    "                                                              'j':tf.FixedLenFeature([],tf.int64),})\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.local_variables_initializer().run()\n",
    "    print(sess.run(files))\n",
    "    \n",
    "    coord=tf.train.Coordinator()\n",
    "    threads=tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    for i in range(6):\n",
    "        print(sess.run([features['i'],features['j']]))\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0] [0 1 0]\n",
      "[0 1 1] [1 0 1]\n"
     ]
    }
   ],
   "source": [
    "files=tf.train.match_filenames_once('Records/data.tfrecords-*')\n",
    "filename_queue=tf.train.string_input_producer(files,shuffle=False)\n",
    "\n",
    "reader=tf.TFRecordReader()\n",
    "_,serialized_example=reader.read(filename_queue)\n",
    "features=tf.parse_single_example(serialized_example,features={'i':tf.FixedLenFeature([],tf.int64),\n",
    "                                                              'j':tf.FixedLenFeature([],tf.int64),})\n",
    "example,label=features['i'],features['j']\n",
    "batch_size=3\n",
    "capacity=1000+3*batch_size\n",
    "\n",
    "example_batch,label_batch=tf.train.batch([example,label],batch_size=batch_size,capacity=capacity)\n",
    "#example_batch,label_batch=tf.train.shuffle_batch([example,label],batch_size=batch_size,capacity=capacity,min_after_dequeue=30)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.local_variables_initializer().run()\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads=tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    \n",
    "    for i in range(2):\n",
    "        cur_exmaple_batch,cur_label_batch=sess.run([example_batch,label_batch])\n",
    "        print(cur_exmaple_batch,cur_label_batch)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), loss is 402.868 \n",
      "After 1000 training step(s), loss is 6.63943 \n",
      "After 2000 training step(s), loss is 6.50159 \n",
      "After 3000 training step(s), loss is 7.24043 \n",
      "After 4000 training step(s), loss is 6.47796 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "files=tf.train.match_filenames_once('Records/output.tfrecords')\n",
    "filemane_queue=tf.train.string_input_producer(files,shuffle=False)\n",
    "\n",
    "reader=tf.TFRecordReader()\n",
    "_,serialized_example=reader.read(filemane_queue)\n",
    "features=tf.parse_single_example(serialized_example,features={'image_raw':tf.FixedLenFeature([],tf.string),\n",
    "                                                              'pixels':tf.FixedLenFeature([],tf.int64),\n",
    "                                                              'label':tf.FixedLenFeature([],tf.int64)})\n",
    "decoded_images=tf.decode_raw(features['image_raw'],tf.uint8)\n",
    "retyped_images=tf.cast(decoded_images,tf.float32)\n",
    "labels=tf.cast(features['label'],tf.int32)\n",
    "\n",
    "images=tf.reshape(retyped_images,[784])\n",
    "\n",
    "min_after_dequeue=1000\n",
    "batch_size=100\n",
    "capacity=min_after_dequeue+3*batch_size\n",
    "\n",
    "image_batch,label_batch=tf.train.shuffle_batch([images,labels],batch_size=batch_size,capacity=capacity,min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "def inference(input_tensor,weights1,biases1,weights2,biases2):\n",
    "    layer1=tf.nn.relu(tf.matmul(input_tensor,weights1)+biases1)\n",
    "    return tf.matmul(layer1,weights2)+biases2\n",
    "\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "LAYER1_NODE = 500\n",
    "REGULARAZTION_RATE = 0.0001   \n",
    "TRAINING_STEPS = 5000        \n",
    "\n",
    "weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "\n",
    "weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "\n",
    "y = inference(image_batch, weights1, biases1, weights2, biases2)\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=label_batch)\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "regularaztion = regularizer(weights1) + regularizer(weights2)\n",
    "loss = cross_entropy_mean + regularaztion\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    tf.local_variables_initializer().run()\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After %d training step(s), loss is %g \" % (i, sess.run(loss)))\n",
    "        sess.run(train_step) \n",
    "    coord.request_stop()\n",
    "    coord.join(threads) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
